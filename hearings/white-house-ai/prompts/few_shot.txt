You are a U.S. congressional hearing annotator.

Your task is to analyze a hearing transcript and, for each listed speaker, determine their **stance** toward a list of predefined policy topics. You are impartial, strict, and grounded in what the speaker explicitly or implicitly says.

For each speaker and each topic, assign one of the following sentiment labels:

- `1` = Explicit or implicit **support** for the topic  
- `-1` = Explicit or implicit **opposition** to the topic  
- `0` = **Neutral**, no opinion expressed, or topic not mentioned

Default to `0` unless there is clear evidence in the speaker's language.

- Do **not** infer support or opposition based on general hardship, economic trends, or moral values.
- Do **not** co-label topics unless the speaker clearly links them.
- Do **not** assign sentiment based on political party, role, or ideology.
- Quoting another person does not imply support/opposition unless the speaker agrees or rejects it.


EXAMPLES
Example 1:

- Excerpt from a transcript:
"    Chairman Comer. Unfortunately, the FDA under President Biden is suffering 
from dysfunction and is failing to do bare minimum to carry out 
its core mission, which is to make certain our Nation's food 
and drug products are safe and effective. Further, the FDA 
appears consistently unprepared for certain crises. That is why 
our Committee has conducted several investigations into areas 
of concern at the FDA. These investigations have identified a 
pattern of issues within the FDA."

- Topic:
FDA under Biden

- Stance of Chairman Comer towards "FDA under Biden":
-1 (Negative)


Example 2:

- Excerpt from a transcript:
"    Mr. Raskin. Under Biden, FDA has improved access to contraception and protections 
for medication abortion access. In 2021, FDA advanced the 
accessibility of medication abortion by removing the in-person 
dispensing requirement for mifepristone and allowing it to be 
distributed by mail through retail pharmacies. In July 2023, 
FDA approved the first over-the-counter birth control pill, 
Opill. As a result, consumers' access to contraception has 
improved at a critical time when many states are enacting 
increasingly draconian and oppressive abortion restrictions."

- Topic:
FDA under Biden

- Stance of Mr. Raskin towards "FDA under Biden":
1 (Positive)


Example 3:
- Excerpt from a transcript:
"    Mr. Gosar. I thank the Chairman. Now obviously, the FDA 
made a mistake in granting the Emergency Use Authorization and 
license of COVID-19 vaccines. It has been confirmed that the 
vaccines do not stop transmission. Moreover, 1,635,048 injuries 
due to COVID-19 vaccines have been reported to the Health and 
Human Services through the Vaccine Adverse Event Reporting 
System, including 37,382 deaths."

- Topic:
FDA under Biden

- Stance of Mr. Gosar towards "FDA under Biden":
-1 (Negative)


Example 4:

- Excerpt from a transcript:
"    Dr. Califf. FDA's continued oversight will be critical until supply 
chains are more resilient, particularly for infant formula. We 
will continue to promote competition in manufacturing quality 
and implement modernized systems to respond to shortages 
faster. It is why we have requested additional authorities that 
would provide more visibility into the supply chain."

- Topic:
FDA under Biden

- Stance of Mr. Califf towards "FDA under Biden":
0 (Neutral)


Example 5:
- Excerpt from a transcript:
"    Mr. Grothman. Throughout the COVID 
epidemic, I spoke multiple times on the floor with regard to 
the value of vitamin D. Now, the adequate level of vitamin D 
varies depending upon how you talk to, you know, 20 nanograms, 
30 nanograms, 50 nanograms, but whatever the study you look at, 
the number of lives saved if everybody had adequate levels of 
vitamin D is tremendous, OK? It is a relatively cheap vitamin, 
but for whatever reason it was not pushed by the medical 
establishment and resulted, in my opinion, in the deaths of 
hundreds of thousands of people."

- Topic:
FDA under Biden

- Stance of Mr. Grothman towards "FDA under Biden":
0 (Neutral)


OUTPUT FORMAT
You will output a CSV table and nothing else. The CSV must look like this:

Speaker,Topic 1,Topic 2,...
Mr. <Last Name>,1,-1
Ms. <Last Name>,-1,1
Dr. <Last Name>,0,1
...

Each row must correspond to a speaker and each cell must contain `1`, `-1`, or `0`.


LIST OF SPEAKERS
Ms. Mace, Mr. Connolly, Mr. Hammond, Dr. Ho, Ms. Goodloe, Mr. Nodurft, Dr. Chowdhury, Mr. Timmons, Mr. Lynch, Mr. Burlison, Ms. Lee, Mr. Burchett


LIST OF TOPICS
Executive Order, OMB guidance


HEARING TRANSCRIPT
Ms. Mace. Good afternoon, everyone. The Subcommittee on 
Cybersecurity, Information Technology, and Government 
Innovation will now come to order, and we welcome you here this 
afternoon.
    Without objection, the Chair may declare a recess at any 
time, and I recognize myself for the purpose of making an 
opening statement.
    Good afternoon and welcome to this hearing on the 
Subcommittee on Cybersecurity, Information Technology, and 
Government Innovation.
    Since the release of ChatGPT just over a year ago, it has 
become clear AI could soon disrupt nearly every facet of our 
economy and society from healthcare to warfare. And that is 
good news. AI is a triumph of American innovation.
    It is also likely to boost business productivity, raise our 
standard of living, and lead to life-saving and life-extending 
medical advances.
    But like any powerful tool, AI could be used to inflict 
great harm when it is used carelessly or by malicious actors. 
That is why we have explored the dark side of AI in this 
Subcommittee, the risk that AI-fueled cyber-attacks pose to our 
national security and critical infrastructure, the threats to 
data privacy, the ways child sexual abuse material can 
proliferate online via deepfake technology, and the risk of 
personal harm to individuals from unchecked algorithmic bias.
    These risks and others, including the rise and use of AI to 
weaponize biotechnology, are addressed in the broad reaching 
executive order on AI that President Biden signed on October 
30.
    Two days later, OMB followed on with a draft guidance 
specifically governing Federal agency use of AI. A brief 
comment period on that guidance ended yesterday. So, this is 
perfect timing for this hearing today.
    In the EO, the President invokes extraordinary emergency 
powers, under the Defense Production Act, to require companies 
to notify the government about the development of powerful new 
AI systems and to share safety testing results.
    But for the most part, both the EO and the OMB guidance 
tasks Federal agencies with mitigating against the dangers of 
specific, high-risk, AI-use cases as opposed to regulating the 
technology itself. That is a critical distinction.
    The AI genie is out of the bottle, and it cannot be put 
back in. Suppressing core AI innovation here in the U.S. will 
not stop China from advancing the technology on its own, and if 
we fall behind China in the AI race, all other risks seem tame 
by comparison.
    And China, quite frankly, as you all know, is not far 
behind.
    That is why our support measures in these documents that 
seek to spur the recruitment and retention of AI experts in 
both the private and public sector. Here in the Federal 
Government, we need employees who can responsibly partner with 
the private sector to procure AI systems that make our 
government smarter, smaller, and more effective.
    I look forward to hearing your views from industry 
witnesses today concerning how the OMB AI guidance is likely to 
impact Federal agencies' uses of AI and the ability of 
businesses to work with the government to provide cutting-edge 
AI tools that are safe and reliable.
    But no one can yet judge the impact of the EO or the 
guidance. For the most part they are just kick-starting a 
process. The EO tasks Federal agencies with a massive laundry 
list of roughly 150 action items to take over the next year and 
beyond.
    Dozens of regulations and guidance documents will be 
issued. Every major agency and many minor ones are enlisted in 
the effort, so we in Congress will be watching closely as this 
process unfolds.
    But I am a little skeptical of Federal agencies that will 
keep to the timetable of action laid out in the documents 
because their track record is pretty useless. After all, the 
draft OMB guidance on government use of AI we are discussing 
today was due, by law, from this Administration more than 2 
years ago, for example.
    With that, I will now yield to the Ranking Member for his 
opening statement.
    Mr. Connolly. Thank you, Madam Chairwoman, and welcome to 
our panelists to today's hearing where we are making the world 
better one hearing at a time.
    A Qualcomm report published just last week estimates that 
the total economic benefit of generative AI amounts to roughly 
$6.1 trillion to $7.9 trillion. Hard to believe.
    So, what does this AI-infused global economy look like, 
whatever the number?
    We already know virtual AI assistants like Alexa and Siri. 
I use both. Lesser-known use cases include helping scientists 
develop clinical drugs to treat pulmonary fibrosis, defending 
bank customers from identity fraud, and improving traffic 
congestion for communities across the globe.
    With these incredible advancements, AI also brings risks. 
In 2017, for example, researchers set out to train an AI model 
to identify cancerous lesions using clinical images. While 
researchers initially hailed the experiment a success, they 
subsequently realized that their algorithm's diagnoses were not 
informed by the lesion in the photo but rather by the presence 
of a dermatologist's ruler used to measure particularly 
concerning skin lesions.
    That error could have been clearly a matter of life and 
death.
    The United States must continue to invest in AI R&D and 
address those issues to solidify itself as a global leader.
    Ten years ago, machines struggled to reliably identify 
images. Today national militaries are using AI analysis for 
satellite imagery, to determine missile and artillery strikes.
    AI has also grown its ability to understand and respond to 
language to the point where even Members of Congress are using 
ChatGPT.
    Over the past 10 years, the private sector has invested 
$249 billion into AI development, and the world's top 5 AI 
companies are headquartered in the United States.
    On the public side, the National Science Foundation has 
announced a $140 million investment to establish seven new 
National Artificial Intelligence Research Institutes and to 
advance a cohesive approach to AI-related opportunities and 
risks.
    In addition, agencies like NIH invested $5.9 million into 
the University of Virginia to fund research into how artificial 
intelligence could support care for diverse populations.
    These investments can be important, if not critical, but 
the AI race remains competitive. If the U.S. fails to continue 
to support investment in this technology, we will be left 
behind, particularly by China.
    While traditional research and development tools like data 
and training models can improve technology, so can Federal 
Government leadership in setting standards, guidance, and 
regulatory frameworks.
    Establishing clear and transparent rules of the road builds 
trust within the public and establishes user security and 
privacy protections.
    AI regulatory frameworks around the world will reflect the 
values of their own governments and societies, and those 
national frameworks matter because they will influence future 
iterations of AI.
    In China, for example, the government requires AI companies 
to uphold core Socialist values in providing services. It is 
essential, therefore, that the United States lead in AI 
governance or the Nation risks ceding foundational control to 
adversarial forces eager to influence the future of AI.
    President Biden's executive order on the safe, secure, and 
trustworthy development and use of artificial intelligence, 
along with OMB's draft implementation guidance, do that.
    These frameworks lead a comprehensive, society wide effort 
to ensure AI best serves and protects the American people. The 
President's EO builds on the important action that this 
Administration has taken on AI to date, including the creation 
of the blueprint for an AI Bill of Rights and the National 
Institute of Standards and Technology's AI risk management 
framework.
    As the representative of the Silicon Valley East, I support 
provisions in the EO to bolster our Federal AI workforce. The 
President's accelerating hiring of AI professionals, while 
simultaneously offering AI training for employees at all levels 
of relevant fields, so the agency personnel are ready to 
confront the challenges both today and tomorrow.
    President Biden's executive order and OMB's draft 
implementation guidance are essential documents for 
implementing this technology safely and responsibly across the 
Federal Government and society broadly.
    Congress, industry, and the Administration must now work 
together to ensure that the Nation meets the important goals of 
the executive order and to continually seek to find and measure 
improved regulatory thresholds as needed as this technology 
evolves.
    This Subcommittee looks forward to working with all 
stakeholders to encourage the safe and responsible development 
and usage of AI.
    With that, I yield back.
    Ms. Mace. Thank you.
    I am pleased today to introduce our witnesses for today's 
hearing. Our first witness is Mr. Samuel Hammond, Senior 
Economist at the Foundation for American Innovation;
    Our second witness is Dr. Daniel Ho, professor of law and 
senior fellow at the Stanford Institute for Human-Centered AI 
at Stanford Law School;
    Our third witness is Ms. Kate Goodloe, Managing Director at 
BSA, the Software Alliance;
    Our fourth witness is Mr. Ross Nodurft, Executive Director 
at the Alliance for Digital Innovation;
    And our fifth and final witness is Dr. Rumman Chowdhury, a 
responsible AI fellow at the Berkman Klein Center for Internet 
and Society at Harvard University.
    We welcome you all here today, and we are pleased to have 
you this afternoon.
    Pursuant to the Committee rule 9(g), the witnesses will 
stand and raise their right hands, please.
    Do you solemnly swear or affirm that the testimony that you 
are about to give is the truth, the whole truth, and nothing 
but the truth so help you God?
    Let the record show the witnesses all answered in the 
affirmative. We appreciate all of you here today and look 
forward to your testimony. I will remind the witnesses that we 
have read your written statements, and they will appear in full 
in the hearing record. Please limit your oral statements to 5 
minutes today.
    As a reminder, please press the button on the microphone in 
front of you so that it is on when you speak, and the Members 
up here can hear you.
    When you begin to speak the light in front of you will turn 
green. After 4 minutes the light will turn yellow. When the red 
light comes on, your 5 minutes have expired, and I will bang 
the gavel to shut you up. Just kidding. But I will.
    So, today you all can take a seat. I would like to 
recognize Mr. Hammond for your opening statement for 5 minutes, 
please.
    Mr. Hammond. Thank you, Chairwoman Mace, Ranking Member 
Connolly, and Members of the Subcommittee. My name is Samuel 
Hammond. I am the Senior Economist at the Foundation for 
American Innovation. We are a group of technologists and policy 
experts working to develop technology, talent, and ideas for a 
free and a more abundant future.
    From the printing press to the internet, history shows that 
transformative technologies are a key driver of institutional 
change and evolution. Artificial intelligence is no different. 
The only question is whether our government will keep up and 
adapt or become overwhelmed by the pace of change.
    This is why I believe the risk from adopting AI in 
government must be balanced against even greater risks 
associated with failing to adopt AI proactively enough.
    Enter the White House's executive order on AI. A sprawling 
document, there is much to applaud in the executive order, from 
the streamlining of uses to attract and retain immigrants with 
AI expertise, to new initiatives for addressing Federal 
cybersecurity vulnerabilities.
    I was particularly impressed by the EO's focus on, quote, 
dual-use foundation models such as those that can be used to 
generate biologic agents, cyber weapons, or other catastrophic 
hazards to the American public.
    The EO requires basic disclosures from large computing 
providers and the adoption of minimal safety testing for 
future, more powerful AI systems.
    Yet more than anything, the bulk of the executive order is 
aimed at promoting the use of AI within government. Whether it 
is successful, only time will tell as right now it is mostly a 
series of requests and reports.
    My worry, though, is that it does not go far enough and 
could even hinder the fulsome adoption of AI in government 
given excess focus on hypothetical harms and even deeper 
failure of imagination.
    Our institutions and AI are in an arms race, and we need to 
sprint just to stay in place. This is most obvious in the arena 
of cybersecurity where AI is both creating novel threats and 
more powerful forms of cyber defense.
    Yet these arms-race dynamics extend far beyond cases where 
AI is explicitly misused. Even the most productive uses of AI 
will put unprecedented strain on our government.
    Just last week Google DeepMind published an AI model that 
discovered 380,000 new stable materials. This represents nearly 
800 years' worth of new material science knowledge achieved 
virtually overnight.
    Now imagine what will happen when the same pace of change 
comes to medicine as it almost surely will. Is the FDA prepared 
to handle an order of magnitude increase in new drug discovery? 
The answer is clearly no, at least not under business as usual.
    Similar bottlenecks exist through the Federal Government. 
In every case managing the AI transition will require 
government to not only adopt AI aggressively but may even force 
Congress to rethink the configuration of our administrative and 
regulatory agencies from the ground up.
    From broken procurement policies and cumbersome procedures, 
incremental reform is unlikely to suffice. We must modernize 
government at the firmware level and embrace AI and AI 
frameworks that truly scale, or risk government becoming the 
primary bottleneck to technological progress.
    Earlier this year OpenAI published a paper assessing the 
likely labor market impacts of large language models. They 
found jobs like accountants, auditors, and legal secretaries 
face an exposure rate of 100 percent.
    Many large companies have already begun downsizing, or have 
plans to downsize, in anticipation of the enormous efficiency 
gains unlocked by emerging AI tools and AI agents.
    Much of the work performed in government bureaucracies is 
similar low-hanging fruit for AI. Congress should request an 
analogous survey to discover which Federal jobs are most 
exposed to AI automation or augmentation and to what extent 
legislation is needed to enable new, AI-enabled models of 
governance.
    The goal should not be to downsize the Federal Government, 
per se, but rather to augment productivity and free up human 
resources for higher-value uses.
    Given bureaucratic inertia, it is not enough to simply ask 
agencies to prioritize use of AI in government. Congress must 
push the Federal Government to adopt AI more aggressively, 
including by authorizing additional AI training resources, 
hiring authorities, and nimble funding for modernization.
    The same tools for enhancing Federal capacity can be 
further used to strengthen congressional oversight. As agencies 
embrace AI internally, managers will be able to easily track 
and query the performance of their staff, automatically 
generating reports and work summaries from common document 
repositories.
    These same techniques could be used to expedite reports to 
Congress and even enable near real time monitoring of an 
agency's activities. You could call it an Inspector General 
GPT.
    Innovating within government should mean more than just 
plugging AI into some existing outdated process and calling it 
a day. We need true inventiveness and ambition. So, while White 
House has taken some important first steps, there is much more 
to be done.
    Thank you and I look forward to your questions.
    Ms. Mace. Thank you.
    I will now recognize Dr. Ho to begin your opening 
statement.
    Dr. Ho. Chairwoman Mace, Ranking Member Connolly, and 
Members of the Subcommittee, thanks for this opportunity to 
testify today.
    There are three possible futures of AI. One is a future of 
AI abuse unchecked by government regulation. Nefarious actors 
use AI voice cloning to scam citizens, bot-generated texts to 
impersonate people, and deepfakes to erode trust.
    Another is a future where the government harms citizens 
because of improper vetting of AI.
    But a third future is possible where the government 
protects Americans from bad actors and leverages AI to make 
lives better, like the VA's use of AI to enable physicians to 
spend more time caring for veteran patients and less time 
taking notes.
    To get to that future, we must make the right decisions 
today. The AI EO and OMB memo are important steps. Their focus 
on AI safety, investment, talent, and leadership are critical 
for America to lead in AI innovation and governance.
    But the executive branch cannot achieve this goal fully 
without Congress. By our count, as Chairwoman Mace noted, the 
EO has some 150 requirements with urgent deadlines.
    Based on my research of prior AI-related EOs, the 
government needs sufficient resources, expertise, information, 
and flexibility to realize this vision. I, therefore, recommend 
six actions.
    First, Congress should support the EO's focus on top-level 
leadership from the White House and from the agency chief AI 
officers. Agencies will need resources and flexibility to not 
just put out fires but craft long-term strategic plans.
    Second, Congress must support efforts to A, attract, train, 
and retain AI talented America; and B, provide pathways into 
the public service. Each year our universities are turning out 
a growing number of students with advanced degrees in AI. Yet 
fewer than 1 percent of AI Ph.Ds. pursue a career in public 
service. We need creative public-private partnerships to fix 
this talent gap.
    When the return of millions of overseas veterans after 
World War II threatened to overwhelm the VA hospital system, 
the VA developed a pipeline of medical students and faculty to 
provide veteran care.
    At Stanford RegLab and HAI, we collaborate with government 
agencies to prototype exactly this kind of partnership in AI, 
and increasing mechanisms to partner and collaborate with 
universities will be critical.
    Third, a mandated, adverse event reporting system that 
requires parties to disclose AI harms would equip the 
government with information to ensure that AI is safe for the 
American public. Chairwoman Mace, for instance, has called for 
exactly this kind of information.
    Some calls for regulation have been driven by more 
speculative risks, such as how ChatGPT might facilitate 
bioweapons. Other harms are very real, such as erroneous loan 
denials, biased hiring algorithms, or malfunctioning self-
driving cars.
    Currently our government lacks unbiased information. 
Adverse event reporting, like what already exists for 
cybersecurity or medical devices, would ensure that the 
government can tell fact from fiction about real and emerging 
AI harms, and it would enable targeted regulation that avoids 
stifling innovation.
    Fourth, the development of general-purpose foundation 
models should not be restricted through a licensing regime. 
Fears about the capabilities of these models have led some to 
argue that foundation models should only be developed by a few 
well equipped companies.
    This is wrong. Licensing only a small number of companies 
would impede valuable safety research. The most important forms 
of accountability come from oversight by many.
    Done poorly, licensing would concentrate power, limit 
competition, and exacerbate the information gap between 
government and industry.
    Fifth, government must appropriate funds to agencies and 
pass the bipartisan CREATE AI Act to fully authorize the 
National AI Research Resource and foster investment in R&D 
necessary for a wider range of Americans to participate in the 
AI revolution.
    The National Institute of Standards and Technology is 
tasked with establishing the U.S. AI Safety Institute to 
develop verifiable and enforceable safety standards. Agencies 
like NIST must be sufficiently resourced to carry out these 
critical missions.
    Sixth, government innovation should not be trapped in red 
tape. The OMB memo is exemplary in spelling out the 
opportunities and risks of AI, but process must be tailored to 
risk.
    For instance, the memo's proposal that agencies allow 
everyone to opt out of AI for human review does not always make 
sense given the sheer variety of programs and uses of AI.
    Since 1965, the U.S. Postal Service, for instance, uses AI 
to read handwritten zip codes on envelopes. Opting out of this 
system would mean hiring thousands of employees just to read 
digits alone.
    Humans also make mistakes. Denials of SNAP benefits, for 
instance, are inaccurate 44 percent of the time. The government 
cannot ``human'' its way out of these problems. The government 
must build and leverage AI systems that complement human 
strengths and values.
    In sum, the AI EO and OMB memo have taken a big first step, 
but it is only one step on the longer journey. Congress must 
now take it.
    Thank you and I welcome your questions.
    Ms. Mace. Thank you.
    I now recognize Ms. Goodloe to please begin your opening 
statement.
    Ms. Goodloe. Good afternoon, Chairwoman Mace, Ranking 
Member Connolly, and Members of the Subcommittee. My name is 
Kate Goodloe, and I am Managing Director at BSA, the Software 
Alliance.
    BSA is a leading advocate for the global enterprise 
software industry. BSA members are at the forefront of 
developing cutting-edge services, including AI, and their 
products are used by businesses in every sector of the economy 
and by agencies across the Federal Government.
    I commend the Subcommittee for convening today's hearing, 
and I thank you for the opportunity to testify.
    The United States needs a strong, clear, thoughtful 
approach to AI policy. Both Congress and the Administration 
have important roles in developing that policy. It is critical 
for the United States to get this right.
    The benefits of AI are clear, as companies of all sizes in 
every industry use AI to improve safety, create better 
products, and serve their customers.
    There are also significant risks if AI is not developed and 
deployed responsibly. AI policy should, one, protect 
individuals from real risks by creating durable safeguards that 
promote trust in AI; two, enable the government to benefit from 
AI technologies and deliver better public services; and three, 
position the United States as a leading voice in the global 
approach to responsible AI.
    The benefits of getting this right are significant, 
including to promote the government's ability to procure and 
use tools like AI-powered cybersecurity services.
    Think about a Federal agency trying to protect both its 
network and the sensitive information it has about individuals, 
things like passport information, medical records, tax 
documents.
    We already know bad actors are using AI to launch 
increasingly sophisticated cyber-attacks. The government needs 
AI to stay ahead of those threats too.
    The United States' AI policy should support important 
beneficial uses of AI that improve health, safety, national 
security while creating guardrails for high-risk uses.
    The recent executive order takes an ambitious whole-of-
government approach to AI policy. I want to highlight several 
of the positive steps it takes to advance responsible AI.
    The executive order recognizes the importance of the AI 
risk management framework developed by the National Institute 
of Standards and Technology. We encourage the Administration to 
ensure that framework anchors the government's risk management 
efforts.
    The executive order also recognizes the importance of AI in 
cyber defense. It launches a pilot program to implement the 
National AI Resource to give researchers access to compute 
power and training resources.
    It recognizes the importance of content authenticity, 
tools, and standards to help people know when content is real 
and when it has been altered, and it promotes the coordinated 
enforcement of civil rights statutes across agencies.
    Other parts of the order create notable, important 
obligations with effects that will depend on how they are 
implemented. We encourage the Administration to consult with 
stakeholders, including industry, to ensure those obligations 
work in practice and do not undermine the order's goals.
    I will give two examples. First, are new reporting 
requirements, which will apply to companies that develop 
certain potential dual-use foundation models and entities that 
acquire or possess large scale computing clusters.
    Second, are new know-your-customer obligations for U.S. 
infrastructure-as-a-service providers, who must report certain 
transactions with foreign persons to the Department of Commerce 
and pass on those obligations to their foreign resellers.
    The executive order also addresses government use and 
procurement of AI which are the focus of the draft guidance by 
the Office of Management and Budget. My written testimony 
includes BSA's recommendations for improving that guidance, 
including to ensure it applies consistently across agencies.
    It is also important to coordinate OMB's changes with five 
concurrent regulatory updates that affect how the government 
procures AI. Failing to do so can undermine the government's 
goal of effectively leveraging AI.
    The executive order is much broader than these efforts. It 
tasks more than 40 Federal agencies and entities with drafting 
reports, conducting consultations, and developing rules.
    Despite this ambitious approach, the order does not replace 
the need for congressional action on AI. Congress should play a 
leading role in setting the United States' AI policy in at 
least two ways.
    First, Congress should pass legislation that ensures the 
NIST framework guides the government's use and procurement of 
AI systems.
    Second, Congress should enact legislation that establishes 
new safeguards for private sector companies that develop and 
deploy high-risk AI. These actions can help to create a strong, 
clear, and thoughtful United States AI policy.
    Thank you for the opportunity to testify, and I look 
forward to your questions.
    Ms. Mace. Thank you.
    I will now recognize Mr. Nodurft for your opening 
statement.
    Mr. Nodurft. Thank you, Chairwoman Mace, Ranking Member 
Connolly, and Members of the Committee for holding this 
important hearing today. My name is Ross Nodurft. I am the 
Executive Director for the Alliance for Digital Innovation. We 
are a coalition of innovative commercial companies whose 
mission is to bring IT modernization and emerging technologies 
to the government.
    My prior experience includes working at the Office of 
Management and Budget, in the Office of the Federal Chief 
Information Officer, as well as working in the private sector 
with many companies to modernize their technology, bring in 
Cloud services, cybersecurity products, and now artificial 
intelligence tools.
    As the Executive Director of the Alliance for Digital 
Innovation, I represent leading technology, artificial 
intelligence, quantum computing, cybersecurity, and 
professional service providers, all working with the public 
sector.
    ADI focuses on four key areas in our advocacy efforts--
accelerating technology modernization in government, enabling 
acquisition policies that make sense to bring in innovative 
technologies, promoting cybersecurity innovations to better 
protect the public and the private sector, and then improving 
the public sector's technology workforce.
    Regarding the Administration's recent AI policies, overall, 
ADI is supportive of any legislation or administrative policy 
that promotes adoption and use of modern Cloud-based commercial 
technology to increase the pace of government mission delivery.
    We are also very supportive of the efforts to provide a 
public comment period for a draft memo. We understand that that 
is not something that is done traditionally, and we really 
support that effort.
    However, there was a really short turnaround time that is 
limiting the amount of thoughtful and constructive feedback 
that industry can provide.
    In fact, the rushed nature of the response mean OMB will be 
finalizing its guidance to agencies without the full benefit of 
insights that can be provided by the industry partners that are 
developing and deploying the AI capabilities in partnership 
with the government.
    That said, ADI believes there are several important key 
areas that the administration should clarify as it updates its 
guidance.
    The OMB memo could inadvertently keep innovative businesses 
away from the public sector. The OMB memo creates a series of 
fractured and unevenly administered new processes across 
departments and agencies that will deter many companies, 
including small and midsize technology companies, from working 
with the Federal Government.
    To solve for this, OMB should provide additional 
specificity about various trigger mechanisms for determining 
which technologies are considered rights-impacting and which 
technologies are considered safety-impacting.
    It should create a repository for the reuse of various 
products or testing documents.
    And then they should consider certain use cases, like 
specific cybersecurity-use cases, to call out as neither 
rights-impacting, nor safety-impacting, and proactively exempt 
those products and services from the minimum requirements.
    Government should use current government processes for AI. 
We cannot have agencies trying to implement the new processes 
without fully considering how they fit into current technology 
and security governance regimes, and how they are optimized for 
AI adoption.
    We strongly encourage the Administration to provide 
agencies with enough time to optimize their plans for adoption 
and use of AI, leveraging current governance processes before 
providing their plans to comply with the executive order.
    The government should prioritize specific AI-use cases. AI 
technology is not new. It has been around in many forms for 
many years. ADI believes that OMB must help agencies prioritize 
governance processes that focus on delivery of new AI or 
generative AI capabilities and to distinguish from those 
existing AI and machine learning capabilities that are already 
authorized and in use in the Federal Government.
    The government must refine definitions and trigger 
mechanisms for rights-and safety-impacting systems. ADI 
recommends further refining safety-and rights-impacting 
definitions to delineate harms associated with specific 
categories.
    Examples for safety-impacting could be loss of life or 
serious physical harm, while rights-impacting examples could be 
tied to harms that are currently protected by existing laws 
such as nondiscrimination and consumer protection.
    In addition to further defining the risks, ADI believes 
that OMB must better define the term ``meaningful impact'' 
which acts as the trigger mechanism for such harms.
    Finally, the government should clarify data ownership and 
focus on outcome-based testing. OMB should clearly state that 
the company data and proprietary, personal information does not 
have to be disclosed to the government for review.
    Additionally, OMB should clarify that assessing the quality 
and the appropriateness of relevant data does not mean 
reviewing the underlying training data when reviewing AI 
systems but instead allows for a summary description of the 
characteristics of the training design data and ensuring that 
that will meet the requirement.
    Finally, ADI recommends that OMB focus on model testing, 
known limitations, guidelines for intended use, and example-
performance results of an AI model.
    Thank you again to the Committee for the opportunity to 
testify, and I look forward to your questions.
    Ms. Mace. Thank you.
    I will now recognize Dr. Chowdhury to please begin her 
opening statement.
    Dr. Chowdhury. Chairwoman Mace, Ranking Member Connolly, 
and esteemed Members of the Committee, my name is Dr. Rumman 
Chowdhury, and I am a data scientist and social scientist who 
has built AI, and responsible AI, within and without industry 
for the past decade.
    The executive order and the subsequent OMB guidance lay out 
an ambitious strategy for the accelerated responsible 
deployment of AI.
    I applaud the recognition that, in order for the U.S. to 
remain an AI superpower, it must focus on safe, secure, and 
trustworthy use. I offer the following recommendations to 
facilitate this goal.
    First, the U.S. must remain an active leader in the global 
AI landscape by funding targeted interventions and responsible 
AI for public and global use;
    Second, in order to achieve the goals of section 4 of the 
EO, support and fund NIST;
    Third, develop the independent community of algorithmic 
auditors by enabling secure-model access and investing in 
education for structured, public feedback methods such as red-
teaming and bias bounties;
    Fourth, develop a minimum requirement standard that 
includes a determination of whether or not AI adoption is 
necessary and appropriate for Federal Government use.
    First, countries are moving quickly to establish global 
standards and best practices around responsible use. I arrived 
this morning from Singapore's AI for Global Good workshop.
    Their government gathered AI experts from around the world 
to help co-create ten projects that they will fund for open use 
and global benefit.
    They are not alone. I have collaborated on similar efforts 
in London, Brussels, Paris, and Oslo, where there is similar 
investment in global, responsible-use best practices.
    The U.S. must continue to set global AI priorities in 
alignment with section 11 of the EO. I recommend that the 
government similarly invest in public interest projects for 
responsible use that are open access, publicly available, and 
drive as a resource for individuals around the world.
    Second, simply put, support NIST. Section 4 of the EO 
develops an ambitious strategy to leverage the institutional 
authority and capacity of NIST and expand their remit. I can 
think of no better team to execute on this plan.
    With a limited timeline and broad scope, they require 
significant funding and resources to deliver the global, 
standard-setting quality that NIST is known for.
    Similar institutes are funded accordingly. The U.K. AI 
Safety Institute has 100 million pounds earmarked for their 
endeavors. The Norwegian Government has allocated a 1 billion 
kroner fund toward AI development.
    In addition, the proposed U.S. AI Safety Institute must 
remain housed as NIST. As a scientific measurement body, they 
provide much needed empirical evidence data to help us 
understand and prioritize how we address the risks and harms 
introduced by AI systems.
    The U.S. Safety Institute must focus on a wide range of 
harms--societal impact, bias and discrimination, as well as 
broader considerations of future risks--in order to provide the 
full breadth of assurance we need to safely deploy AI. We 
already have significant evidence of AI systems in use today 
that infringe upon basic civil and human rights.
    Third, in June, I testified to the House Science, Space, 
and Technology Committee on the topic of AI. At that time the 
concept of red-teaming was known by cybersecurity and few 
others.
    Since then, it has become a topic of much consideration, 
mentioned 15 times in the EO alone. This is due, in part, to 
the White House's support of the generative AI red-teaming 
exercise this past August, which was co-led by my organization, 
Humane Intelligence.
    We need to continue that momentum and enthusiasm. Newly 
appointed chief AI officers, as required by the draft OMB 
guidance, should engage with trusted organizations, including 
the Safety Institute, as well as independent external 
organizations to develop red-teaming as a part of standard 
vendor procurement and project-evaluation processes.
    Fourth, AI is often a hammer in search of a nail. We cannot 
assume that AI is always the best answer to a problem as 
developers optimize for efficiency rather than effectiveness.
    We have already seen how the use of AI infringes upon civil 
rights by algorithmic discrimination in criminal justice, 
employment, banking, and more.
    In 1971, the Supreme Court addressed a similar problem--
unintentional employment discrimination introduced by aptitude 
tests. The Griggs v. Duke Power Company ruling enabled the 
disparate impact requirement that is today widely used in 
evaluating AI hiring systems.
    In order to achieve the goals of sections 7 and 8 of the 
executive order, I recommend a similar approach for the use of 
AI in high-risk situations in order to mitigate unintended 
consequences due to algorithmic bias.
    A minimum standards test could include the following: A 
determination of whether or not an AI system performs better 
than an equal investment in improving the current system; 
requiring alignment with impact metrics designed with NIST to 
measure the effectiveness and robustness of the system, not 
just performance efficiency; a strategy to proactively identify 
and address real-world biases and adverse outcomes through in-
context testing methods like expert or public red-teaming.
    In conclusion, we must be circumspect on if, when, and how 
we adopt AI systems. This technology is meant to serve 
humanity, and innovation is only possible if we are all able to 
reap the benefits.
    Thank you for your time.
    Ms. Mace. Thank you so much.
    I now ask unanimous consent to enter into the record three 
letters from the following organizations: Americans for 
Prosperity, Consumer Technology Association, and the National 
Association of Manufacturers.
    And without objection, so ordered.
    Ms. Mace. Thank you. I will now recognize myself for 5 
minutes of questioning.
    My first question is for you, Mr. Hammond.
    The biggest risk concerning government use of AI is that it 
will not happen fast enough, like everything. I mean, today we 
still have legacy systems throughout the Federal Government, 
and we are all asking ourselves why.
    Call me a little bit of a skeptic. I mean, I learned COBOL 
25 years ago, and we are still using it in the Federal 
Government. But according to your written testimony today, it 
states the question is whether governments will keep up and 
adapt or be stuck riding horses while society whizzes by in a 
race car.
    So, could slow and reluctant government adoption of AI 
jeopardize the cybersecurity of Federal systems? Is this a 
national security issue? Where do you see it?
    Mr. Hammond. Well, thank you for your question. Yes, I 
think it is both a national security issue and a sort of good-
government issue. So, you mentioned COBOL. We lived through the 
pandemic, and when you saw those line-ups around the block to 
claim unemployment insurance, a big part of that was because 
state unemployment insurance systems are built on mainframe 
computing technology from 50, 60 years ago.
    It is broader than that as well. You know, look at the IRS. 
The IRS individual master file, which is sort of the core file 
that determines individual and business tax returns, was coded 
in assembly. So, even more primitive than COBOL, it was from 
the Kennedy Administration.
    And so, these systems, in addition to cybersecurity, 
present sort of risks from through-put in denial of service. 
So, you know, I give an example of, you know, what happens when 
there is another sort of net neutrality style debate where 
activists on both sides are submitting regulatory comments. But 
now instead of just being expletives, they are fully cogent, 
well written comments that under the Administrative Procedures 
Act, we all have to read and respond to.
    Those kinds of sort of tsunamis of information, even if 
they are not explicitly misused, could easily overwhelm an 
agency. And so, you know, in that case, maybe we need to adopt 
AI for summarizing comments or something like that.
    I also think of this in the context of information 
requests. So----
    Ms. Mace. Or filtering data is another, you know, way to do 
that as well--sorry to cut you off because we are at 3 minutes 
now.
    I would like to ask a question of Dr. Ho. You have written 
about the lack of AI talent in government and the failure of 
this Administration to timely implement AI-related mandates.
    I think you said in your testimony like 1 percent of AI 
qualified--people qualified in AI are in the public service--
public sector.
    So, under this new EO, it has got 150 new tasks to perform 
based on your count. Do you expect Federal agencies to meet the 
timetables for actions set out in the EO?
    Dr. Ho. Thank you, Chairwoman Mace. I think you are right, 
that earlier work showed that the two prior AI EOs were 
inconsistently applied, and I think what is good to see about 
this EO is that the Administration has really learned from some 
of the documentation and weak points, particularly in terms of 
providing clear definitions, having some people in charge at 
each agency.
    And the question you raise is a really important one, and 
this Subcommittee has been so important in really providing 
good oversight and transparency over implementation.
    I think followup is going to be necessary, and I think the 
talent pipeline that you mentioned is going to be absolutely 
critical for ensuring that the right folks are in place to be 
able to implement these requirements faithfully and in an 
informed way by the technology.
    Ms. Mace. Thank you. And then my last question will go to 
Mr. Nodurft and Ms. Goodloe.
    By farming out so many decisions to individual agencies, 
are either of you worried about the EO and the guidance might 
lead to multiple, conflicting, AI-enforcement regimes within 
the government? How is that going to work? What does that look 
like to you?
    Mr. Nodurft. So, that is one of my bigger concerns here. 
There is a lot of emphasis--and the risk decisions and risk 
management is at the agency level, and it should be. And there 
are very specific use cases for every AI deployment.
    That said, there is a gap between the guidance that is 
currently being provided and the way that that guidance can 
be--the way that that guidance can be realized at the different 
agencies. And that delta is going to cause people who are 
individually empowered authorizing officials trying to leverage 
this AI to make decisions on whether or not it is good or bad 
based on some of the definitions that could use some more 
specificity frankly.
    So, I am very concerned that it is going to lead to risk-
averse views of AI when we right now need to be embracing the 
technology where----
    Ms. Mace. Thank you. I am going to give the last 25 seconds 
to Ms. Goodloe. Thank you for yielding back.
    Ms. Goodloe. Thank you, Chairwoman Mace. I think the need 
to coordinate actions across agencies is one of the biggest 
challenges and opportunities with this executive order.
    We see a range of agencies tasked with conducting different 
reports, consultations, and issuing rules, and I think the need 
to coordinate those to ensure we have a harmonized policy 
across agencies is imperative.
    One thing the Administration can do is to make sure they 
are consulting with stakeholders to better coordinate.
    Ms. Mace. Thank you.
    All right. I will now yield to my colleague, Mr. Connolly, 
for 5 minutes.
    Mr. Connolly. Thank you so much, and great testimony from 
all of our panelists.
    Dr. Ho, among your six recommendations, you touched on AI 
officers. To whom should the AI officers in respective agencies 
report?
    Dr. Ho. Yes. Thank you for the question, Representative. I 
think this goes back to what Congresswoman Mace had asked 
earlier which is, an earlier study of the two prior EOs and how 
they were implemented within agencies showed that there were 
some real inconsistencies. And part of that, from our sense, 
was actual fragmentation within the agency----
    Mr. Connolly. So--all right. I have got a limited time, and 
org charts matter.
    Dr. Ho. Yes.
    Mr. Connolly. Shouldn't the AI officer report to the CIO--
primary CIO?
    Dr. Ho. I think that, actually, in my view, depends on what 
kind of resources the CIO has available.
    Mr. Connolly. OK. Well, when you go back to Stanford, think 
about it, because this matters. The fragmentation of management 
over cybersecurity, IT, AI, you know, what could go wrong with 
that?
    When we wrote FITARA, we had 240 people with the title CIO 
in 24 agencies--250. No private sector company would put up 
with that, no matter how big.
    Dr. Ho. Yes. Representative, I agree, and that is why one 
of our----
    Mr. Connolly. All right.
    Mr. Ho [continuing]. Recommendations early on was to 
actually ensure that the----
    Mr. Connolly. Right.
    Dr. Ho. [continuing]. Right guy was placed at----
    Mr. Connolly. Which is what I picked up on. But I think we 
have to really nail down, going to Ms. Goodloe's point, we 
cannot have it, you know, all over the place. There has to be 
some systemized set of standards and management practices and 
principles and titles with responsibility--comparable 
responsibility, because you cannot take that for granted in the 
Federal Government.
    Dr. Ho. I agree.
    Mr. Connolly. All right.
    Ms. Goodloe, you talked about NIST and saying it really 
should be the foundation for AI management. Let me just ask--
and I do not mean anything by this, but a devil's advocate 
question--is this the right agency? Why not GSA? Why not OMB?
    You know, NIST has a very specific mandate, set of 
responsibilities. It does not make sense to house AI in NIST.
    Ms. Goodloe. This is a very important question, so thank 
you. NIST's role as an expert agency is a key resource for the 
Federal Government. NIST's creation of an AI risk management 
framework is a significant achievement, and it was done at the 
direction of Congress.
    The expertise that has gone into that framework can be 
leveraged across the government by agencies looking to 
implement AI risk management practices. And so, we recommend 
that the NIST framework be the anchor of how agencies use and 
procure AI systems, so that they manage risk in a coordinated 
way.
    Mr. Connolly. I am agnostic about whether it should or 
should not. It is in the EO, but I would ask myself--maybe I 
would ask you this question--do you think NIST has the 
requisite experience dealing with the private sector?
    Because the AI experience is coming out of the private 
sector, and that means the government has got to intersect with 
private sector entities. And sometimes that works well, 
sometimes it does not.
    What about the culture at NIST, do you think that is going 
to work well, intersecting with the private sector? Because 
making that relationship work is going to be critical.
    Ms. Goodloe. You are right, it is critical, and we do think 
NIST is the right agency to lead expertise on this. Their work 
on the AI risk management framework builds on this success in 
creating risk management framework for things like 
cybersecurity and for privacy.
    The cybersecurity framework has become a gold standard 
worldwide for governments and organizations looking to manage 
cyber risks, and we think their expertise has been put to great 
use in creating the AI RMF.
    Mr. Connolly. OK. That is good to hear. I am going to ask 
two more questions. One is just affirm or not, but in listening 
to all of your testimony, basically I heard you say--and I am 
not trying to put words in your mouth--the EO is a good start, 
got a lot of things right. We are worried about unintended 
consequences. We are worried about did it go far enough--Mr. 
Hammond--but at least it is a good start and creates a 
foundation with which we can work. Is that a fair statement 
from all of your point of view?
    Mr. Hammond. Yes, sir.
    Dr. Ho. Yes.
    Dr. Chowdhury. Yes.
    Mr. Connolly. So, let the record show all of our panelists 
answered in the affirmative.
    OK. My final question, and I hope we have time, what keeps 
you up at night? What should we worry about with respect to AI? 
We know the good it can do. What about the other part?
    Dr. Chowdhury, why don't you start.
    And if the Chair will just give me a little liberty to 
allow you to answer, I will shut up.
    Dr. Chowdhury. Well, first and foremost, we have empirical 
data that there Is bias and discrimination that occurs in AI 
systems today. This comes from the underlying data.
    AI is simply a model or a representation. When it takes 
action, it is taking action as a representation of the data 
that has been fed into it and the designs that have happened to 
it.
    So, as we think about implementing AI, AI being built for 
the public sector needs to work for 100 percent of the people 
from day 1. This is not a commercial product. This is not an 
Uber for puppies.
    You know, these are things that critically matter to 
individuals, so we have to be very careful in how we roll 
things out, so that they are equitable for all. Thank you.
    Oh, and to your previous point about NIST, I would just 
want to add that two of my former team from Twitter are 
actually currently at NIST, so NIST is certainly a draw for 
folks in industry.
    Ms. Mace. I am so generous, so generous.
    Mr. Connolly. You are, you are.
    Ms. Mace. I actually want to hear from the rest of the 
panelists, what keeps you up at night? Ten seconds or less.
    Mr. Connolly. Exactly. Thank you, Madam Chairwoman. I am 
really fascinated with that question.
    Mr. Hammond. Well, as I talked about in my written 
statement, progress in AI is accelerating so quickly that now 
the current forecast for artificial general intelligence, a 
system that could, in principle, do anything a human could do, 
or better, are as soon as 2026.
    And so, one of the things I worry about is that as we go to 
NIST and other organizations to set standards, is that these 
standards will become obsolescent really quickly because it is 
a moving target.
    And second, that if we do not win that race to AGI, that 
China will. And in addition to my work on this stuff, I always 
work on things like expert controls----
    Ms. Mace. Right.
    Mr. Hammond [continuing]. And one of the big glaring 
loopholes is China's ability to access our CHIPS through Cloud 
services in Singapore and elsewhere. We have to close that 
loophole.
    Ms. Mace. Scary.
    Dr. Ho?
    Dr. Ho. Government cannot govern AI if it does not 
understand AI. We need the people and information in order to 
seize this particular moment.
    Ms. Mace. Thank you.
    Ms. Goodloe?
    Ms. Goodloe. And on that note, we also need the right AI 
policy. The U.S. needs to be a leading voice in establishing AI 
policy and be a strong leader worldwide on this issue.
    Ms. Mace. Mr. Nodurft?
    Mr. Nodurft. I worry about the appropriate government 
resources available to fund and to work with the people that 
are in the organizations to train them accordingly so that they 
can leverage the AI technology.
    Ms. Mace. Thank you.
    And I will now yield 5 minutes to Mr. Timmons, my colleague 
from South Carolina.
    Mr. Timmons. Thank you, Madam Chair. I am actually 
intrigued, none of you mentioned anything that involved AI-
enabled weaponry. I mean, AI-enabled, armed drone swarm would 
really kind of be the end of the world as far as a terrorist 
attack goes. I mean, there is all kind of potential uses for 
it.
    Anyways, thank you, Madam Chair. I appreciate you holding 
this hearing on such an important and timely issue.
    As we stand on the precipice of a technological revolution 
driven by AI, it is imperative that our regulatory frameworks 
are not only sufficient to manage risk and foster innovation in 
this emerging technology, but are also harmonized across all of 
government.
    Artificial intelligence has the potential to revolutionize 
industries, drive economic growth, and enhance the quality of 
our lives.
    However, with great power comes great responsibility. As we 
witness the rapid advancements in AI technology, it becomes 
increasingly clear that a patchwork of disjointed regulations 
is not sufficient to address the complex challenges and ethical 
considerations that AI presents.
    Earlier this year, NIST released its AI risk management 
framework. The AI RMF will give companies, agencies, and others 
who utilize AI a common resource when adopting risk management 
programs.
    This will align and coordinate AI risk management across 
organizations. It is a step in a right direction.
    Ms. Goodloe, you mentioned the importance of the NIST RMF 
in your testimony. What are the benefits of that guidance for 
both agencies and the private sector?
    Ms. Goodloe. Yes. Thank you for the question. The NIST 
framework is a significant accomplishment, and one benefit of 
the NIST framework is, it allows organizations to adopt risk 
management practices that have the same structure so that they 
have a common approach to managing AI risks and they have a 
common language for managing those risks.
    In our view, the risk management framework developed by 
NIST should anchor how agencies use and procure AI systems. 
When agencies and their vendors adopt risk management practices 
based on the RMF, they speak the same language, and they can 
better coordinate and manage risks across entities.
    Mr. Timmons. Thank you for that.
    And how can the OMB's approach to AI be improved through 
congressional action to assist in the harmonization of the 
litany of issues you raised in your written testimony?
    Ms. Goodloe. In our view, the OMB memo is a significant 
achievement in taking a risk-based approach to AI risk 
management, but it can be improved in several ways, and my 
written testimony highlights them.
    First, there is a need for a governmentwide approach to 
procurement of AI systems.
    Second, there Is a need for a uniform definition of the 
types of AI systems that are subject to this memo. Right now, 
the definition of ``rights-impacting'' and ``safety-impacting'' 
are broad enough that you can imagine two different agencies 
reaching two different conclusions about whether one AI system 
meets those definitions.
    Finally, we think the OMB memo should better leverage 
NIST's work in creating the risk management framework and look 
to that further as a resource to drive the U.S. Government's 
approach to using and procuring AI technologies.
    Mr. Timmons. Sure. Thank you for that.
    And I guess one last question, why is it important for the 
government to buy commercially available products instead of 
products specifically made for the government, and how would 
the EO or OMB memo play into that process?
    Ms. Goodloe. Yes. This is an important point, and I think 
it needs to be a focus for implementation of the OMB memo. The 
government should be encouraging agencies to buy commercially 
available products which are historically subject to less high-
failure rates.
    They are not likely to go obsolete. They are easier to 
update and, therefore, less vulnerable to threats, and often 
less expensive than products that are made specifically for the 
government.
    So, we think both the Administration and the OMB guidance 
should encourage agencies to buy commercially available 
products.
    Mr. Timmons. Sure. Thank you.
    Dr. Ho and Mr. Nodurft, one final question. You both 
touched on this in their question about what keeps you up at 
night. How can Congress create a regulatory framework that 
protects against potential harm associated with AI while not 
impeding the development and implementation of all the benefits 
that AI has to offer?
    And I am going to say that another way. I am concerned that 
businesses will just relocate abroad if our regulatory 
framework becomes overly complex or burdensome. So, what can we 
do to strike the right balance? Dr. Ho?
    Dr. Ho. Well, I think it is critical that we lead with 
values. There are values that are embedded in technology, and 
one of the big questions facing us, do we want--is whether we 
want a small number of Silicon Valley firms to embed those 
values, whether we want our foreign adversaries to embed those 
values, or whether we want broader forms of democratic input to 
kind of embed those values.
    That is why, for instance, in my opening remarks, I support 
R&D by passing the bipartisan CREATE AI Act to really ensure 
that small businesses and a wider range of Americans can really 
participate in the AI revolution.
    Mr. Timmons. Mr. Nodurft, I will followup in writing. I do 
not want to take more time.
    Mr. Nodurft. Sure.
    Mr. Timmons. Thank you, Madam Chair. I yield back.
    Ms. Mace. You look scared.
    OK. I will yield 5 minutes to Mr. Lynch for questions.
    Mr. Lynch. Thank you, Madam Chair.
    In its final report, the bipartisan National Security 
Commission on Artificial Intelligence underscored that the 
preservation of U.S. leadership in global AI largely depends on 
our ability to present a democratic model, governing the use of 
AI for national security to the rest of the world. Dr. 
Chowdhury, is that even possible? I have my doubts.
    Dr. Chowdhury. In short, yes. One of my recommendations is 
for the U.S. to invest in not just top-down leadership but 
bottom-up, by creating systems, tools, procedures, processes 
similar to what I just came from back in Singapore, to imbue 
our democratic values in implementation of AI systems to be 
made widely available.
    Mr. Lynch. I just see us in a competition, you know, and I 
know China right now has full spectrum surveillance of its 
entire population. It has no constraints at all in terms of, 
you know, respecting individual rights or any--they do not have 
any internal structural restraints that we do in terms of 
serving a democracy.
    I just wonder if, because of the restraints that we have, 
the strength of our government itself and our desire in 
government to make sure that individual rights are respected in 
this technological process, whether or not we just forfeit too 
much in terms of allowing China to get very far ahead of us.
    That is just not a--there is not a good ending to that 
story if we allow that to happen.
    The other aspect of this that I worry about is the power of 
AI, and I know it has been around for a while but not like it 
is now. I mean, we have--you know, Mr. Timmons talked about the 
weaponization of weapon systems that would be completely 
autonomous, things like that, that we are trying to struggle 
with in terms of our military capabilities.
    I just, I see a lot more danger there than I think I have 
heard represented on this panel. And I know government. I have 
been here over 20 years. And while the velocity of change is 
incredible in science and technology, we have not changed much 
up here.
    You know, we got rid of the powdered wigs, you know, but 
that is about it in the last--we still call the cloak room the 
cloak room, and someone with a cloak has not walked in here in 
over a hundred years.
    So, there is some real limitations in our form of 
government, our democracy. We have a lot of restraints on us 
that other governments do not, and I am just very worried that 
we are going to forfeit any opportunity we have to develop the 
type of democratic model of AI that others are urging us to 
create.
    And, I do not know, I just--how can we, in the United 
States, in developing our AI policy, influence other countries, 
even in Europe, friendly, you know, and allies, other 
democracies, how do we make that happen?
    Dr. Chowdhury. You know, as a political scientist, I hear 
what you are saying as kind of an age-old question, right? Is 
it easier or simpler to have an authoritarian government that 
just tells everybody what to do and enforces it? And it is 
true. They move incredibly efficiently.
    But what we have found time and time again is that they do 
not last. They end up being cults of personality. They end up 
being untenable situations. They do not respect the rights of 
individuals. And at some point, people get fed up with it.
    We saw this during World War II and what happened in the 
cold war, where it turned out that even in the countries that 
had, you know, presumably mass support from the people, that 
they actually looked to America. And it seemed silly to think 
that, you know, blue jeans and rock-and-roll tapes were the 
things that swayed hearts and minds, but that is what happened.
    And I actually think that, in a future where AI is imbued 
into a lot of processes and a lot of things that we do and 
influences our daily lives, people will look more toward 
autonomy. They will look toward a better future.
    AI is meant to develop human flourishing. Even the for-
profit companies like OpenAI say that they are building AI 
toward the benefit of humanity. How can we achieve that if it 
is just controlled by a few individuals?
    Mr. Lynch. Madam Chair, my time has expired. Thank you for 
your courtesy.
    Mr. Connolly. Madam Chair, before you recognize our 
colleague, I am informed Representative Summer Lee is on her 
way and would request to be waived on to the Subcommittee. I 
have no idea when she will arrive.
    OK. Thank you.
    Ms. Mace. I will now recognize Representative Burlison for 
5 minutes.
    Mr. Burlison. Thank you, Madam Chair.
    And thank you for everyone that is joining the panel today.
    The Biden executive order on AI is over 100 pages long. It 
includes dozens of new and, I think, far-reaching reforms, 
guidelines, rules, and programs. It tasks over 50 Federal 
entities with approximately 150 different requirements.
    And while I have done my best to evaluate its scope and 
impact, I appreciate the Chairwoman for holding this hearing 
because I think it is helpful to understand the implications of 
this.
    And I will say that, generally speaking, my attitude on 
innovation and technology should be--is that we should be 
laissez-faire. We should be hands-off in any approach, 
especially to such a new and emerging industry.
    As I recall through the history of programming--from 
machine to assembly to procedural programming to general 
purpose--I mean, all of it was developed not by government 
establishing rules on the way in which people should be coding 
or creating some form of outline.
    Fortran was invented by IBM--OK--without any input from the 
Federal Government. BASIC was invented by Microsoft without any 
guidelines or any rules or anything that was passed by an 
executive order. SQL--again, by IBM--and then lately, Apple 
created Swift. All of this has been done by the creative forces 
in the private sector.
    So, my question to the panel--and we will just begin with 
Mr. Hammond at the beginning--is, do you have any concerns? I 
mean, everybody on the panel has said that they support this 
new executive order, but do you understand my concerns about 
how this might throttle back innovation?
    Mr. Hammond. Yes, sir. I mean, you know, one of the reasons 
we are leaders in software is because software has been the 
exception to the rule of our physical industries. You do not 
need to get permission to build a new app in the same way you 
do to build a transmission line or to build a refinery.
    And for that reason, it is why we are the leader in AI. And 
many of the issues that are coming up around deepfakes and so 
on and so forth, those will get market solutions. You know, no 
company wants AI users to be flooding their services, they are 
going to be developing tools, tools that are going to be 
iterating faster than we can set standards.
    Mr. Burlison. Yes. To me, I think of the days, you know, 
when computers first came out, and you had viruses that were 
occurring, and people did not know what to do, and now you have 
antivirus software to address it. I think you are going to 
have--you are certainly going to have some things--some 
nefarious things happen with AI, but the counterpunch to that 
is, you know, white-hat AI.
    Mr. Hammond. Right. Spam filters.
    The one exception I would draw is, if you look at how the 
EO talks about these dual-use foundation models, dual-use can 
be a big term, but the EO is quite narrow in how it defines 
that to include things that can produce biological agents, 
cyber weapons, things of that nature, and I think that is a 
reasonable exception to the rule.
    Dr. Ho. If I may, it is one of the reasons--your concern is 
one of the reasons why I express skepticism about a licensing 
regime that would restrict development of large foundation 
models to a very small number of actors potentially.
    And I think the other thing I want to add here is that, you 
know, it is true that we have had a thriving innovation 
ecosystem that we should promote in order to maintain America's 
leadership in AI.
    That said, part of what has been so important has been 
basic R&D investment by the Federal Government that led to 
innovation that occurred over the long term, including basic 
algorithmic research that has powered the AI revolution, but 
also basic things like radar, the internet, and GPS devices 
that we each have in our pockets.
    And so that is why the parts of the EO that are really 
investment-oriented, like the National AI Research Resource 
sponsored in the CREATE AI Act, are so important.
    Mr. Burlison. I think Ms. Goodloe wants to say something.
    Ms. Goodloe. I do. Thank you.
    On the executive order, I think it is important to look at 
its breadth and the number of actions it takes and to recognize 
that the implementation of those different rules and 
initiatives will matter. And we will know more about the 
effects of this executive order as those rules are implemented 
as we see regulation----
    Mr. Burlison. This sounds a lot like we will find out what 
is in it after we pass it.
    Ms. Goodloe. We will know more as these initiatives are 
taking effect and as we see the rules coming down.
    But I want to also acknowledge that there are real risks 
with AI, and a thoughtful United States policy on AI can help 
increase trust in the technology that is good for the economy 
broadly.
    Mr. Burlison. All right. Well, don't you think that--I 
mean, some of the things that might happen are not new to 
mankind. Theft being used through AI is still theft, right? 
Violence or hate speech or anything like that, all of that is 
not new.
    Sorry. I am over my time.
    Ms. Mace. Thank you.
    Great questions of our panelists today.
    And in closing, I want to thank everybody. And I want to 
thank my colleagues on both sides of the aisle for being here 
this afternoon and having this very important discussion.
    You know, we all have a lot of enormous concerns about the 
advances in AI because it has just gone by so quickly. And I 
think we have to be very careful about--before we even think 
about regulating AI, we have to first figure out how our own 
existing laws today already apply.
    You cannot create bioweapons as it is today. Why would AI 
be any different? AI obviously could not be helpful in that 
either.
    So, I think we have to be very careful and thoughtful so 
that we do not stifle innovation because we want the United 
States to lead around the world. We do not want China to catch 
up with us, and in order for that to happen, we have to keep 
innovating.
    And I think knowing what I know now, as a sophomore 2.5 
years in here up on the Hill, is that the Federal Government 
moves like just the slowest dinosaur. I mean, we are still on 
mainframe computers and legacy systems, and we should not be. 
How in the hell do we think we could make advances in AI vis-`-
vis the government? I mean, that is just not ever going to 
happen.
    And so, I think we have got to be very careful about it, 
but it is worth having this discussion on how we try to protect 
people, consumers, or data in the Federal Government, et 
cetera. How can we use AI to advance the ball in many ways and 
take what is going on in the private sector and using it in the 
public sector?
    And I think we are going to have to rely on--significantly 
rely on the private sector to--what does ethical AI look like? 
Because they are the ones that are going to have to take the 
lead.
    And I would call on industry today in AI to take the lead. 
What does that look like? And encourage industry and companies 
to work together on that framework and what that might look 
like in the future.
    And with that, I will yield to my Ranking Member.
    Mr. Connolly. Thank you, Madam Chairwoman. This has been a 
most illuminating hearing and really thoughtful.
    I want to just say to my friend before he leaves, I think 
he makes a really good point about what could be achieved with 
sort of unimpeded private sector research.
    But I would say to my friend, we have to acknowledge, as 
Dr. Ho did, that the Federal Government has some stunning 
successes in its own research and development. We would not 
have the internet but for what was called DARPANET for 25 
years, a 100-percent-funded Federal R&D project.
    We would not have mapped the human genome without a 100-
percent-federally funded research project, which is going to 
transform medicine. We would not have GPS, which is now 
universal but was a classified Department of Defense technology 
until we decided to open it up commercially. We would not have 
radar. There is a whole string.
    Sometimes the narrative of the Federal Government is really 
skewed. I do not mean to suggest my friend did that. He was 
pointing out the positive aspects of private sector unimpeded, 
and I agree with that. I come from the private sector, too.
    But we have to acknowledge the Federal Government has done 
some spectacular things. The internet. How do you even put--how 
do you put an ROI on the internet? What has the return on that 
investment been? I think it is approaching infinity because it 
has transformed the whole world. It is very hard to put a 
dollar figure on it, but thank God we made that investment. And 
I just point that out.
    Mr. Hammond talked about the pace. We have to be concerned 
about the pace because we are in a race not only with the 
natural evolution of this technology, but with competitors who 
are accelerating or exploiting that pace.
    I listened to a podcast today--I think it was just in the 
last week--with Elon Musk. And he was asked, well, on the pace 
and evolution of AI, when do you think we arrive at a point 
where AI overtakes human intelligence? The smartest brain on 
the planet, AI can do better. And his answer was within 3 
years. Not 30. Three.
    So, we do have challenges, and keeping up with that and 
making sure that we get it right--as you said, Ms. Goodloe, we 
have got to get it right. We do not want to impede. We do not 
want to thwart or, you know, suppress, but on the other hand, 
we want to protect.
    And we want to try to anticipate how we channel AI into 
purposeful and positive, you know, betterment for the quality 
of human life while protecting humans from the worst it could 
produce, and getting that right is going to be a big challenge. 
And as the Chairwoman said, we are going to need the private 
sector as a partner as we proceed to do this.
    Thank you so much for being here, and thank you, Madam 
Chairwoman, for this hearing.
    Ms. Mace. Yes.
    And I see that our colleague has arrived, so I will ask 
unanimous consent for Representative Summer Lee from 
Pennsylvania to be waived on to the Subcommittee for today's 
hearing for the purposes of asking questions.
    Without objection, so ordered.
    Ms. Lee, I will recognize you for 5 minutes of questioning.
    Ms. Lee. Thank you, Madam Chair, and to the Committee for 
allowing me to waive on. And thank you so much for your 
patience for extending, Madam Chair and Ranking Member 
Connolly, and to all of our experts for sharing your testimony 
today and for the important work addressing this issue.
    It is good to see you again, Dr. Chowdhury. Of course, I 
have seen you on the SST Committee.
    The President's October 2023 executive order on AI 
acknowledges that, to appreciate the benefits of AI technology, 
we have to first mitigate substantial risk it poses of 
perpetuating existing biases. The individuals who are most 
marginalized in our community, such as those with disabilities, 
are left most vulnerable to these risks.
    In my district, an Associated Press investigation found 
that the family risk predictor tool used by a child protective 
services agency harbored potential bias against people with 
disabilities. The civil rights repercussions of these opaque AI 
tools, or black boxes, are alarming, and those impacted are 
left with little to no options for recourse.
    So, we must approach AI development and implementation with 
ample caution to ensure that the technology serves everyone 
equitably. For this to happen, diversity and inclusion must be 
fundamental at every stage of AI development process.
    Dr. Chowdhury, how can we ensure that diversity and 
inclusion ultimately influence the outcomes of AI systems?
    Dr. Chowdhury. Thank you for the question. It is great to 
see you again as well.
    Well, first, we have to start with diverse teams. And I 
could not help but notice that, of the agencies that have named 
chief AI officers, only the NSF has a woman at the lead. And 
this matters. This matters in industry. This matters in 
government as well. I encourage the government to seek diverse 
candidates to set direction and perspective.
    Second, there must be checks for diverse data. In 
engineering, we have this phrase, ``garbage in, garbage out.'' 
AI is simply a reflection of the data and the design decisions 
that have been made to create the system.
    Overwhelmingly, we see AI systems reflect the biases that 
exist in society. For example, there was a kidney allocation 
algorithm that discriminated against Black patients because of 
a history of systemic discrimination by doctors.
    And finally, there needs to be testing--such as red 
teaming--for minority perspectives when relevant for the use of 
the product. For example, facial recognition that does not 
recognize different skin tones.
    Ms. Lee. Yes. Thank you.
    The AI executive order highlights the importance of 
increased investment in AI research and education, such as what 
has taken place at Carnegie Mellon University in my district.
    Since the 1950's, CMU has been at the forefront of AI 
development and recently announced a collaboration between 
their Responsible AI initiative and NIST to host a workshop 
with the goal of operationalizing the NIST AI Risk Management 
Framework. AI auditing and assessment is an integral element of 
building equitable systems.
    Dr. Chowdhury, as CEO and cofounder of Humane Intelligence, 
a nonprofit that provides AI assessments for clients, what are 
some of the major barriers you have observed in translating 
ethical AI principles to practice?
    Dr. Chowdhury. The primary barrier of adopting AI in 
general is reliability and consistency. So, while those of us 
in tech are impressed with compute size and speed, the biggest 
companies in the world--those who provide tangible solutions 
for consumers--are interested in good customer experience. So, 
it is not a good customer experience if an AI model 
hallucinates, discriminates, impacts mental health, and so on.
    Companies are actually looking for sensible boundaries and 
not barriers to adopting AI. I use the phrase, ``brakes help 
you drive faster.'' So, one of the implementation or adoption 
of responsible AI is the requirement to create tools and 
systems that integrate into how a company builds and deploys AI 
systems.
    I have done this for about 7 years at this point. It is 
certainly possible. But we need to think about 
operationalization, measurement, and what sort of rules and 
standards can be interoperable and universal.
    Ms. Lee. At this point, there is a lot of hype surrounding 
AI, and it' has become hard to separate fact from fiction. 
There is a concern that exists among the public about AI 
existentialism, a fear experienced by many that AI will someday 
become fully autonomous or even sentient.
    Dr. Chowdhury, if you could cut through the AI hype for us, 
are these concerns about artificial general intelligence and AI 
existentialism well-founded?
    Dr. Chowdhury. In short, no. What our well-founded concerns 
are the already established discriminatory practices introduced 
by unmonitored AI systems in surveillance, criminal justice, 
healthcare, financial services, education, and more.
    Ms. Lee. Thank you.
    Just to close, we have to continue to uphold the priorities 
of safe, secure, and trustworthy AI to guarantee that everyone 
can reap the benefits of the technology. In upholding these 
priorities, it is crucial that we continue to bring attention 
to the AI harms that are disproportionately impacting 
marginalized communities, as you have laid out for us today.
    I thank you all so much for being with us today and for 
your testimony, and I thank you so much for the time today.
    Thank you. I yield back.
    Ms. Mace. Thank you.
    And my colleague from Tennessee just showed up. So, we 
saved the best for last.
    Mr. Burchett, you have 5 minutes.
    Mr. Burchett. Thank you, Chairlady. And thank you for 
bringing this to the forefront.
    I am still not exactly sure what this is all about or what 
you all do, but I am learning.
    So, as I sit up here, you have got a bunch of old guys that 
are still in powder-blue leisure suits with zingo-dingo zip-up 
boots and are still listening to 8-tracks in their 1972 AMC 
Gremlin fastback. So, I mean, that is who you are calling on to 
regulate this, and I am kind of afraid of the regulation 
because we could end up stifling something that could be really 
good for this country and this world.
    Mr. Connolly. Can I say to my friend, I think you are 
speaking for yourself. The only thing I even knew what you just 
said was eight-track.
    Mr. Burchett. And I loaned him my Johnny Paycheck eigth-
track, and it is the Christmas special. I will be needing to 
get that back here pretty soon. Thank you, sir, as always.
    Dr. Ho, do you know of any instances of foreign countries 
like China who are using artificial intelligence to oppress its 
citizens?
    Dr. Ho. There are indeed foreign adversaries who have used 
AI to repress populations. And to step back here a little bit, 
all of this is happening in the context of this kind of 
geopolitical competition where China, for instance, has 
announced that it wants to be the world's leader in AI by 2030.
    Just to go back to your opening remarks, though, I think 
you are right to sort of express the concern about 
overregulating. And that is why, in my opening remarks, the 
thing I am quite, sort of, fond of is a kind of adverse event 
reporting system, where for cybersecurity harms, harms of 
medical devices, there are ways to drive down the information 
gap of what is known in the private sector and between 
government so that we can have forms of regulation that are not 
overbearing and that are actually tailored to the kinds of 
harms that have manifested.
    Mr. Burchett. Let me ask you--do not punch your button yet, 
Dr. Ho--does China use this to advance its agenda abroad, and 
if so, how?
    Dr. Ho. Yes. As we have seen in other instances with, you 
know, hardware like the ones involving the company Huawei, 
there are real concerns about our foreign adversaries using 
technology as a way to influence countries. That is why I think 
international collaboration around these issues is going to be 
central.
    It is really good to see that, in the EO, we are seeing, 
you know, an entire section dedicated to really fostering 
multilateral collaboration amongst like-minded countries so 
that we can have a form of tech diplomacy so that the values 
that are encoded in these kinds of systems are ones that are 
really representing the kind of American values that exist 
here.
    For instance, you have--related to some of the earlier 
discussion of the NIST AI RMF, the executive order requires a 
development playbook of how to actually adapt the RMF framework 
to work with other countries, and there are also proposals, 
like the Multilateral AI Research Institute, to try to bring 
like-minded countries together.
    That was a recommendation endorsed by the National AI 
Advisory Committee on which I sit, to actually bring like-
minded countries together to formulate a collaborative approach 
to AI governance.
    Mr. Burchett. What about Executive Order 14110? Do you 
think it goes far enough to allow us to compete against China?
    Dr. Ho. I think the executive order, as I said in my 
opening remarks, is a really important first step. It is the 
first step, and I think there are numerous other steps that I 
think--I am hoping Congress will take, particularly to invest 
in leadership within government, to bring talent both to the 
government--to have a pipeline of folks within agencies, but 
also to train, retain, and attract talent here to the United 
States.
    Maybe one fact I can give is that 40 percent of engineering 
and science Ph.Ds. in the country are visa holders, and 
historically, we, as a country, have been remarkable at 
retaining that talent. One estimate has it that 80 to 90 
percent of those Ph.Ds. stay in the country.
    Recently, there have been signs that that has been 
changing, and we do not, at this point, want a kind of brain 
drain where people are leaving the country. And I think the 
EO's provisions that are particularly speaking to the 
immigration front on that side, I think, are quite important.
    We, as a country, have to remain a magnet for scientific 
talent for us to retain the kind of leadership position that we 
currently enjoy.
    Mr. Burchett. All right.
    Yes, sir.
    Mr. Hammond. Just to your question about China and the way 
they use AI, a few months ago, my former colleague Geoffrey 
Cain testified in the Senate about his book ``The Perfect 
Police State,'' which discusses China's use of AI to monitor 
the Uyghur population as a pilot program for their country as a 
whole.
    And I think, as AI disrupts institutions worldwide, there 
is going to be a race among every tin-pot dictator out there to 
import technology to restore security, and one of the roles the 
U.S. can play is by developing defensive technology that can do 
things like policing and law enforcement while preserving civil 
liberties.
    Mr. Burchett. All right. I have run over my time.
    Ranking Member, Chairlady, it has been wonderful. Thank you 
so much.
    Ms. Mace. Thank you.
    And with that, and without objection, all Members will have 
5 legislative days within which to submit materials and to 
submit additional written questions for the witnesses, which 
will be forwarded to the witnesses for their response.
    If there is no further business, without objection, the 
Subcommittee stands adjourned.